{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNETcorrrect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7mASaKEY45PqXXniR/zDi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeMarica/XLNET-text-classification/blob/main/XLNETcorrrect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install SentencePiece\n",
        "!pip install transformers==4.10.0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0Z1KUPj4HRA",
        "outputId": "742954bf-1c4f-4b65-e2b4-857a9f1e4ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting SentencePiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: SentencePiece\n",
            "Successfully installed SentencePiece-0.1.96\n",
            "Collecting transformers==4.10.0\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (3.4.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (4.62.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (4.10.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.10.0) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 48.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers==4.10.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.10.0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.10.0) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.10.0) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.10.0) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVLSYm2G4MXW",
        "outputId": "1e321081-44f0-47bd-c6ed-c612977f1a9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import os\n",
        "import sentencepiece\n",
        "from tqdm import tqdm, trange\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer)"
      ],
      "metadata": {
        "id": "t7O28MV7xRsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.read_csv(\"/content/gdrive/MyDrive/Colab Notebooks/XLNET/data/train.csv\", sep=\",\", encoding=\"utf-8\", usecols=['user_suggestion', 'user_review'])\n",
        "tokenizer = XLNetTokenizer(vocab_file='/content/gdrive/MyDrive/Colab Notebooks/XLNET/models/xlnet-base-cased/xlnet-base-cased-spiece.model', do_lower_case=False)\n",
        "model_file_address = '/content/gdrive/MyDrive/Colab Notebooks/XLNET/models/xlnet-base-cased'\n",
        "xlnet_out_address = '/content/gdrive/MyDrive/Colab Notebooks/XLNET/models/xlnet_out_model'"
      ],
      "metadata": {
        "id": "fOjkFd0cxZhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag2idx = {'0': 0, '1': 1}\n",
        "max_len = 256\n",
        "batch_num = 16\n",
        "epochs = 4"
      ],
      "metadata": {
        "id": "HfXMnoVNxu--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb8K_MEM3yeH"
      },
      "outputs": [],
      "source": [
        "sentences = df_data.user_review.to_list()\n",
        "labels = df_data.user_suggestion.to_list()\n",
        "tag2name = {tag2idx[key]: key for key in tag2idx.keys()}\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "full_input_ids = []\n",
        "full_input_masks = []\n",
        "full_segment_ids = []\n",
        "\n",
        "SEG_ID_A = 0\n",
        "SEG_ID_B = 1\n",
        "SEG_ID_CLS = 2\n",
        "SEG_ID_SEP = 3\n",
        "SEG_ID_PAD = 4\n",
        "\n",
        "UNK_ID = tokenizer.encode(\"<unk>\")[0]\n",
        "CLS_ID = tokenizer.encode(\"<cls>\")[0]\n",
        "SEP_ID = tokenizer.encode(\"<sep>\")[0]\n",
        "MASK_ID = tokenizer.encode(\"<mask>\")[0]\n",
        "EOD_ID = tokenizer.encode(\"<eod>\")[0]\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    tokens_a = tokenizer.encode(sentence)   # Tokenize sentence to token id list\n",
        "    if len(tokens_a) > max_len - 2:       # Trim the len of text\n",
        "        tokens_a = tokens_a[:max_len - 2]\n",
        "    tokens = []\n",
        "    segment_ids = []\n",
        "    for token in tokens_a:\n",
        "        tokens.append(token)\n",
        "        segment_ids.append(SEG_ID_A)\n",
        "    tokens.append(SEP_ID)                   # Add <sep> token\n",
        "    segment_ids.append(SEG_ID_A)\n",
        "    tokens.append(CLS_ID)                   # Add <cls> token\n",
        "    segment_ids.append(SEG_ID_CLS)\n",
        "    input_ids = tokens\n",
        "    input_mask = [0] * len(input_ids)       # The mask has 0 for real tokens and 1 for padding tokens. Only real tokens are attended to.\n",
        "    if len(input_ids) < max_len:            # Zero-pad up to the sequence length at fornt\n",
        "        delta_len = max_len - len(input_ids)\n",
        "        input_ids = [0] * delta_len + input_ids\n",
        "        input_mask = [1] * delta_len + input_mask\n",
        "        segment_ids = [SEG_ID_PAD] * delta_len + segment_ids\n",
        "    assert len(input_ids) == max_len\n",
        "    assert len(input_mask) == max_len\n",
        "    assert len(segment_ids) == max_len\n",
        "    full_input_ids.append(input_ids)\n",
        "    full_input_masks.append(input_mask)\n",
        "    full_segment_ids.append(segment_ids)\n",
        "\n",
        "\n",
        "tags = [tag2idx[str(lab)] for lab in labels]\n",
        "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(full_input_ids, tags, full_input_masks,full_segment_ids,\n",
        "                                                                                                  random_state=4, test_size=0.3)\n",
        "tr_inputs = torch.tensor(tr_inputs)\n",
        "val_inputs = torch.tensor(val_inputs)\n",
        "tr_tags = torch.tensor(tr_tags)\n",
        "val_tags = torch.tensor(val_tags)\n",
        "tr_masks = torch.tensor(tr_masks)\n",
        "val_masks = torch.tensor(val_masks)\n",
        "tr_segs = torch.tensor(tr_segs)\n",
        "val_segs = torch.tensor(val_segs)\n",
        "\n",
        "\n",
        "# Set token embedding, attention embedding, segment embedding\n",
        "train_data = TensorDataset(tr_inputs, tr_masks,tr_segs, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks,val_segs, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))\n",
        "model.to(device)\n",
        "max_grad_norm = 1.0\n",
        "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs\n",
        "# Fine tune model all layer parameters\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0}]\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n",
        "\n",
        "model.train()\n",
        "print(\"***** Running training *****\")\n",
        "print(\"  Num examples = %d\" % (len(tr_inputs)))\n",
        "print(\"  Batch size = %d\" % (batch_num))\n",
        "print(\"  Num steps = %d\" % (num_train_optimization_steps))\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        print(step)\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_segs, b_labels = batch\n",
        "        # forward pass\n",
        "        outputs = model(input_ids=b_input_ids, token_type_ids=b_segs, input_mask=b_input_mask, labels=b_labels)\n",
        "        loss, logits = outputs[:2]\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
        "\n",
        "\n",
        "model_to_save = model.module if hasattr(model, 'module') else model\n",
        "output_model_file = os.path.join(xlnet_out_address, \"pytorch_model.bin\")\n",
        "output_config_file = os.path.join(xlnet_out_address, \"config.json\")\n",
        "torch.save(model_to_save.state_dict(), output_model_file)\n",
        "model_to_save.config.to_json_file(output_config_file)\n",
        "tokenizer.save_vocabulary(xlnet_out_address)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OiAdwiNzmz7",
        "outputId": "20b09c56-5b49-4f9b-a3fb-b2326371a1a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at /content/gdrive/MyDrive/Colab Notebooks/XLNET/models/xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.weight', 'lm_loss.bias']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at /content/gdrive/MyDrive/Colab Notebooks/XLNET/models/xlnet-base-cased and are newly initialized: ['sequence_summary.summary.weight', 'logits_proj.bias', 'sequence_summary.summary.bias', 'logits_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 12245\n",
            "  Batch size = 32\n",
            "  Num steps = 1532\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  25%|██▌       | 1/4 [11:40<35:00, 700.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.3559126779046982\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  50%|█████     | 2/4 [23:20<23:20, 700.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.21577257328027985\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  75%|███████▌  | 3/4 [35:00<11:40, 700.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.13373874326928942\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 4/4 [46:39<00:00, 699.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.09823485159014074\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/gdrive/MyDrive/Colab Notebooks/XLNET/models/xlnet_out_model/spiece.model',)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = XLNetForSequenceClassification.from_pretrained(xlnet_out_address,num_labels=len(tag2idx))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "y_true = []\n",
        "y_predict = []\n",
        "print(\"***** Running evaluation *****\")\n",
        "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
        "print(\"  Batch size = {}\".format(batch_num))\n",
        "for step, batch in enumerate(valid_dataloader):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_segs, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=b_input_ids, token_type_ids=b_segs, input_mask=b_input_mask, labels=b_labels)\n",
        "        tmp_eval_loss, logits = outputs[:2]\n",
        "    # Get textclassification predict result\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    # Save predict and real label reuslt for analyze\n",
        "    for predict in np.argmax(logits, axis=1):\n",
        "        y_predict.append(predict)\n",
        "    for real_result in label_ids.tolist():\n",
        "        y_true.append(real_result)\n",
        "\n",
        "report = classification_report(y_pred=np.array(y_predict), y_true=np.array(y_true))\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smsbCEJLz73x",
        "outputId": "e4a73aff-b8ad-4cb4-c932-edfe4bd13799"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** Running evaluation *****\n",
            "  Num examples =5249\n",
            "  Batch size = 32\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.91      0.88      2212\n",
            "           1       0.93      0.88      0.90      3037\n",
            "\n",
            "    accuracy                           0.89      5249\n",
            "   macro avg       0.89      0.90      0.89      5249\n",
            "weighted avg       0.90      0.89      0.89      5249\n",
            "\n"
          ]
        }
      ]
    }
  ]
}