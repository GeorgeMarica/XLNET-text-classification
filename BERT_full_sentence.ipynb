{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_full_sentence.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeorgeMarica/XLNET-text-classification/blob/main/BERT_full_sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd0bT8M09Yxx"
      },
      "source": [
        "Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK14waLM9cLl"
      },
      "source": [
        "!pip install bert-for-tf2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Ox_h10OhGx"
      },
      "source": [
        "import bert\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers  import Input, GlobalAveragePooling1D, Dense, Conv1D, Dropout, LSTM, Bidirectional\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VN9IhVqIct4h"
      },
      "source": [
        "BERT model and tokenizer definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXYcDM53V05P"
      },
      "source": [
        "def build_model(max_seq_length, max_no_categories, bert_layer):\n",
        "  input_word_ids = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "  input_mask = Input(shape=(max_seq_length,), dtype=tf.int32,name=\"input_mask\")\n",
        "  segment_ids = Input(shape=(max_seq_length,), dtype=tf.int32,name=\"segment_ids\")\n",
        "  pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
        "  x = Bidirectional(LSTM(units=150, activation='tanh', dropout=0.2))(sequence_output)\n",
        "  out = Dense(max_no_categories, activation=\"softmax\", name=\"dense_output\")(x)\n",
        "  model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
        "  model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQbqZQhXUTkE"
      },
      "source": [
        "def build_tokenizer(bert_layer):\n",
        "  FullTokenizer=bert.bert_tokenization.FullTokenizer\n",
        "  vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "  tokenizer=FullTokenizer(vocab_file)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGDja69vc-_X"
      },
      "source": [
        "BERT full sentence embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9IaN5_Fkt7o"
      },
      "source": [
        "def get_masks(tokens, max_seq_length):\n",
        "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    segments = []\n",
        "    current_segment_id = True\n",
        "    for token in tokens:\n",
        "        segments.append(int(current_segment_id==True))\n",
        "        if token == \"[SEP]\":\n",
        "            current_segment_id = not(current_segment_id)\n",
        "    current_segment_id = not(current_segment_id)\n",
        "    return segments + [int(current_segment_id==True)] * (max_seq_length - len(tokens))\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n",
        "    return token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "\n",
        "def create_single_input(sentence,max_seq_length,tokenizer):\n",
        "  stokens = tokenizer.tokenize(sentence)\n",
        "  stokens = stokens[:max_seq_length-2]\n",
        "  stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n",
        "  return get_ids(stokens, tokenizer, max_seq_length), get_masks(stokens, max_seq_length), get_segments(stokens, max_seq_length)\n",
        "\n",
        "def create_input_array(sentences, max_seq_length, tokenizer):\n",
        "  input_ids, input_masks, input_segments = [], [], []\n",
        "  for sentence in sentences:\n",
        "    ids,masks,segments=create_single_input(sentence,max_seq_length, tokenizer)\n",
        "    input_ids.append(ids)\n",
        "    input_masks.append(masks)\n",
        "    input_segments.append(segments)\n",
        "  return [np.asarray(input_ids, dtype=np.int32), np.asarray(input_masks, dtype=np.int32), np.asarray(input_segments, dtype=np.int32)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h5nvqG8Qe7O"
      },
      "source": [
        "Model evaluation and saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhCU0yi6TYKH"
      },
      "source": [
        "def model_evaluation(test_x, test_y, model):\n",
        "  yhat_probs = model.predict(test_x, verbose=0, batch_size=64)\n",
        "  yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "  yhat_probs = yhat_probs[:, 0]\n",
        "  eval_dict={}\n",
        "  eval_dict.update({'accuracy':accuracy_score(test_y, yhat_classes)}) # accuracy: (tp + tn) / (p + n)\n",
        "  eval_dict.update({'precision':precision_score(test_y, yhat_classes, average='weighted')}) # precision tp / (tp + fp)\n",
        "  eval_dict.update({'recall':recall_score(test_y, yhat_classes, average='weighted')}) # recall: tp / (tp + fn)\n",
        "  eval_dict.update({'F1 score':f1_score(test_y, yhat_classes, average='weighted')})\n",
        "  eval_dict.update({'Cohens kappa':cohen_kappa_score(test_y, yhat_classes, weights='linear')})\n",
        "  eval_dict.update({'Confusion_matrix':confusion_matrix(test_y, yhat_classes)})\n",
        "  return eval_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJ4PB18KGx4l"
      },
      "source": [
        "def save_model(model, model_save_name):\n",
        "  model.save_weights('/content/gdrive/My Drive/drug_data/' + model_save_name + '.h5')\n",
        "  with open('/content/gdrive/My Drive/drug_data/' + model_save_name + '.json', 'w') as f:\n",
        "    js=model.to_json()\n",
        "    f.write(js)\n",
        "  f.close()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_r7wZnmeJuV"
      },
      "source": [
        "MAIN EXECUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS5qOV-J9jjI"
      },
      "source": [
        "Import data and initialize static parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alMe4ekRYblD"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0pQjBHAFvpIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMLNhG4UxErD"
      },
      "source": [
        "def truncate_text(text, max_seq_length):\n",
        "    return ' '.join(text.split()[:max_seq_length])\n",
        "epochs = 5\n",
        "max_seq_length=256\n",
        "model_save_name = 'bert_full_sentence'\n",
        "text_column = 'user_review'\n",
        "classes_column = 'user_suggestion'\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/XLNET/data/train.csv', sep=',')\n",
        "df['short'] = [truncate_text(text, max_seq_length) for text in df[text_column].values.tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQyoxsQrTobT"
      },
      "source": [
        "Train, evaluate, save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FamI9-T8K32n"
      },
      "source": [
        "x = df['short'].values\n",
        "y = df[classes_column].values.astype(int)\n",
        "max_no_categories = np.amax(y)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXw_6CYiT9KQ"
      },
      "source": [
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiyyTH-pMyUt"
      },
      "source": [
        "bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/2\", trainable=True)\n",
        "bert_tokenizer = build_tokenizer(bert_layer)\n",
        "train_inputs = create_input_array(train_x, max_seq_length, bert_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoAVwwxs1Z9U"
      },
      "source": [
        "bert_model =  build_model(max_seq_length, max_no_categories,bert_layer)\n",
        "bert_model.fit(train_inputs, train_y, epochs=epochs, batch_size=16,validation_split=0.3,shuffle=True, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0xRdAKsQlAe"
      },
      "source": [
        "test_inputs = create_input_array(test_x, max_seq_length, bert_tokenizer)\n",
        "yhat_probs = bert_model.predict(test_inputs, verbose=0, batch_size=32)\n",
        "yhat_classes = np.argmax(yhat_probs,axis=1)\n",
        "report = classification_report(y_pred=yhat_classes, y_true=test_y)\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wGTi6P4HrGw"
      },
      "source": [
        "save_model(bert_model, model_save_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooNGz50Pf2D0"
      },
      "source": [
        "# with open('/content/gdrive/My Drive/drug_data/bert_full_sentence.json') as f:\n",
        "#   bert_model = model_from_json(f.read())\n",
        "# bert_model.load_weights('/content/gdrive/My Drive/drug_data/bert_full_sentence.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}